{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision\n",
    "![title](data/cv_heading.jfif)\n",
    "### In this tutorial, we are going to learn how to take a photo with a webcam attached to our Pynq Board, display this image in our Jupyter Notebook, and then detect faces in the photo. For extra credit, we will use a neural net to predict the objects in a photo.\n",
    "\n",
    "## What is Computer Vision?\n",
    "\n",
    "Computer vision is a field of artificial intelligence (AI) that enables computers and systems to derive meaningful information from digital images, videos and other visual inputs â€” and take actions or make recommendations based on that information. If AI enables computers to think, computer vision enables them to see, observe and understand.\n",
    "\n",
    "Computer vision works much the same as human vision, except humans have a head start. Human sight has the advantage of lifetimes of context to train how to tell objects apart, how far away they are, whether they are moving and whether there is something wrong in an image.\n",
    "\n",
    "Computer vision trains machines to perform these functions, but it has to do it in much less time with cameras, data and algorithms rather than retinas, optic nerves and a visual cortex. Source [https://www.ibm.com/topics/computer-vision]\n",
    "\n",
    "Examples of Computer Vision:\n",
    "1. Self-driving cars. Ex: Tesla ![alt text](data/cv_tesla.png)\n",
    "2. Irrigation management ![alt text](data/cv_irrigation.jfif)\n",
    "3. Plant monitoring ![alt text](data/cv_plant_monitoring.jpg)\n",
    "4. Mask detection (Pandemic) ![alt text](data/cv_mask.jpg)\n",
    "5. Monitoring social distancing ![alt text](data/cv_social_distance.jpg)\n",
    "6. List goes on and on........\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Overlay\n",
    "Our PYNQ Board has a Field Programmable Gate Array (FPGA) on it that must be programmed before we begin using the board. We apply designs called overlays that we can design however we want. For the first part of this tutorial, we are going to use the pre-built base overlay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynq.overlays.base import BaseOverlay\n",
    "from pynq.lib.video import *\n",
    "base = BaseOverlay(\"base.bit\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import Libraries\n",
    "Here we tell the board what libraries we want to use in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Initialize Webcam\n",
    "After plugging in our USB webcam, we must tell the board what size images it is going to be recording and sending to the board. We also need to create a python object that will store the images we read from the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create object\n",
    "videoIn = cv2.VideoCapture(0)\n",
    "\n",
    "#set input width and height\n",
    "input_frame_width = 640\n",
    "input_frame_height = 480\n",
    "videoIn.set(cv2.CAP_PROP_FRAME_WIDTH, input_frame_width);\n",
    "videoIn.set(cv2.CAP_PROP_FRAME_HEIGHT, input_frame_height);\n",
    "\n",
    "#check to ensure the webcam is open\n",
    "if(videoIn.isOpened()):\n",
    "    print('camera is ready')\n",
    "else:\n",
    "    print('error starting camera, run this cell again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Take Photo\n",
    "Once we have our board setup and webcam working, it is time to use it! using videoIn.read(), we can read what the webcam is looking at at any given moment. This function has two return values. The first tells us if we successfully read an image or not. True means we did, false means there was a problem. The second return value is a frame object, which is the actual image we read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read frame\n",
    "success, frame = videoIn.read()\n",
    "\n",
    "#if there was an error, tell us!\n",
    "if (success != True):\n",
    "    print(\"Video Read Error\")\n",
    "else:\n",
    "    print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Display Photo\n",
    "Using matplotlib imported earlier, we can display our image right in our Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display input\n",
    "plt.imshow(frame[:,:,[2,1,0]])\n",
    "#plt.imshow(frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Look for Faces\n",
    "Now that we can take a photo, let's have our board look for faces in our photo. We are going to use something called a Haar Cascade Classifier. This is a model that has been training to understand what a face looks like. It has already been built and trained, so we simply need to load it and pass our image to it. Below is what the model is looking for when it tries to find faces, can you trick it?\n",
    "![HaarClassifier](data/haar.jpg)\n",
    "\n",
    "#### A little more on Haar Classifier: \n",
    "It is an Object Detection Algorithm used to identify faces in an image or a real time video. The algorithm uses edge or line detection features. The algorithm is given a lot of positive images consisting of faces, and a lot of negative images not consisting of any face to train on them. \n",
    "\n",
    "![HaarClassifier](data/cv_haar_gif.gif)\n",
    "\n",
    "Image source: https://towardsdatascience.com/face-detection-with-haar-cascade-727f68dafd08 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1- create our own instance of the classifier\n",
    "face_cascade = cv2.CascadeClassifier('/home/xilinx/jupyter_notebooks/base/video/data/haarcascade_frontalface_default.xml')\n",
    "\n",
    "#2- our classifier prefers to work on a gray image\n",
    "#so we convert our image to grayscale\n",
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#3- Let's see what the grayscale image look like\n",
    "plt.imshow(gray[:,:])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Show our image again with the face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1- we detect face(s) in our picture\n",
    "faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "if faces is ():\n",
    "    print(\"No faces found !\")\n",
    "\n",
    "#2- draw a rectangle around any faces we find\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),3)\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = frame[y:y+h, x:x+w]\n",
    "    \n",
    "#3- Now we see our picture with face(s) detected   \n",
    "plt.imshow(frame[:,:,[2,1,0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Look for Eyes\n",
    "The board also have another classifier for detecting features that look like eyes. We will do the same thing we did with the face classifier for the eye classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1- create our own instance of the classifier\n",
    "eye_cascade = cv2.CascadeClassifier('/home/xilinx/jupyter_notebooks/base/video/data/haarcascade_eye.xml')\n",
    "\n",
    "#2- detect eye(s) in our image (if we haven't found faces above, we will not find eyes!)\n",
    "eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "\n",
    "if eyes is ():\n",
    "    print(\"No eyes found !\")\n",
    "\n",
    "for (ex,ey,ew,eh) in eyes:\n",
    "    cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(255,255,0),3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Show image with eye and face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(frame[:,:,[2,1,0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Counting Faces\n",
    "The faces our program found are contained in an array. This stores the location of the faces within the image. If we want to count the faces found in the image, we can simply print the length of this array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(faces))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Detecting Faces in a Video\n",
    "\n",
    "Let us reinitialize the camera one more time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create object\n",
    "videoIn = cv2.VideoCapture(0)\n",
    "\n",
    "#set input width and height\n",
    "input_frame_width = 640\n",
    "input_frame_height = 480\n",
    "videoIn.set(cv2.CAP_PROP_FRAME_WIDTH, input_frame_width);\n",
    "videoIn.set(cv2.CAP_PROP_FRAME_HEIGHT, input_frame_height);\n",
    "\n",
    "#check to ensure the webcam is open\n",
    "if(videoIn.isOpened()):\n",
    "    print('camera is ready')\n",
    "else:\n",
    "    print('error starting camera, run this cell again')\n",
    "videoIn.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "now we look for faces in a video ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "import ipywidgets as widgets\n",
    "import threading\n",
    "\n",
    "#1- create our own instance of the classifier\n",
    "face_cascade = cv2.CascadeClassifier('/home/xilinx/jupyter_notebooks/base/video/data/haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "# 2- Stop button\n",
    "# ================\n",
    "stopButton = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Stop',\n",
    "    disabled=False,\n",
    "    button_style='danger', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Description',\n",
    "    icon='square' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "# 3- Display function\n",
    "# ================\n",
    "def view(button):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    display_handle=display(None, display_id=True)\n",
    "    i = 0\n",
    "    while True:\n",
    "        _, frame = cap.read()\n",
    "        \n",
    "        #face detection\n",
    "        if frame is not None:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "            for (x,y,w,h) in faces:\n",
    "                cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),3)\n",
    "        \n",
    "            frame = cv2.flip(frame, 1) # if your camera reverses your image\n",
    "            _, frame = cv2.imencode('.jpeg', frame)\n",
    "            display_handle.update(Image(data=frame.tobytes()))\n",
    "        if stopButton.value==True:\n",
    "            cap.release()\n",
    "            display_handle.update(None)\n",
    "\n",
    "\n",
    "# Run\n",
    "# ================\n",
    "display(stopButton)\n",
    "thread = threading.Thread(target=view, args=(stopButton,))\n",
    "thread.start()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Guess the object with a neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network?! :)\n",
    "![HaarClassifier](data/cv_nn.webp) \n",
    "\n",
    "Neural networks reflect the behavior of the human brain, allowing computer programs to recognize patterns and solve common problems in the fields of Artificial Intelligence (AI), machine learning, and deep learning.\n",
    "Also known as artificial neural networks (ANNs) or simulated neural networks (SNNs). Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.\n",
    "\n",
    "Artificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Import a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BOARD'] = 'Pynq-Z2'\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! sudo pip3 install git+https://github.com/Xilinx/QNN-MO-PYNQ.git\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install git+https://github.com/Xilinx/QNN-MO-PYNQ.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qnn\n",
    "from qnn import Dorefanet\n",
    "from qnn import utils\n",
    "import os, pickle, random\n",
    "from PIL import Image\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Take Another Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Initialize Webcam\n",
    "After plugging in our USB webcam, we must tell the board what size images it is going to be recording and sending to the board. We also need to create a python object that will store the images we read from the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create object\n",
    "videoIn = cv2.VideoCapture(0)\n",
    "\n",
    "#set input width and height\n",
    "input_frame_width = 640\n",
    "input_frame_height = 480\n",
    "videoIn.set(cv2.CAP_PROP_FRAME_WIDTH, input_frame_width);\n",
    "videoIn.set(cv2.CAP_PROP_FRAME_HEIGHT, input_frame_height);\n",
    "\n",
    "#check to ensure the webcam is open\n",
    "if(videoIn.isOpened()):\n",
    "    print('camera is ready')\n",
    "else:\n",
    "    print('error starting camera, run this cell again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success, frame2 = videoIn.read()\n",
    "\n",
    "#if there was an error, tell us!\n",
    "if (success != True):\n",
    "    print(\"Video Read Error\")\n",
    "    \n",
    "#display the image\n",
    "plt.imshow(frame2[:,:,[2,1,0]])\n",
    "plt.show()   \n",
    "\n",
    "#save the image just taken to memory\n",
    "cv2.imwrite(\"photo.jpg\", frame2)\n",
    "\n",
    "videoIn.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Set Up the Object Classifier\n",
    "This classifier has been trained to recognize a wide variety of objects. It requires us to reconfigure the FPGA on the board (replacing the base overlay with a custom overlay that will help accelerate the classification). This is automatically done with the init_accelerator() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Dorefanet()\n",
    "classifier.init_accelerator()\n",
    "net = classifier.load_network(json_layer=\"/usr/local/lib/python3.6/dist-packages/qnn/params/dorefanet-layers.json\")\n",
    "\n",
    "conv0_weights = np.load('/usr/local/lib/python3.6/dist-packages/qnn/params/dorefanet-conv0.npy', encoding=\"latin1\").item()\n",
    "fc_weights = np.load('/usr/local/lib/python3.6/dist-packages/qnn/params/dorefanet-fc-normalized.npy', encoding='latin1').item()\n",
    "\n",
    "with open(\"/home/xilinx/jupyter_notebooks/qnn/imagenet-classes.pkl\", 'rb') as f:\n",
    "    classes = pickle.load(f)\n",
    "    names = dict((k, classes[k][1].split(',')[0]) for k in classes.keys())\n",
    "    synsets = dict((classes[k][0], classes[k][1].split(',')[0]) for k in classes.keys())\n",
    "    \n",
    "conv0_W = conv0_weights['conv0/W']\n",
    "conv0_T = conv0_weights['conv0/T']\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Run the Image through the Classifier\n",
    "Once our classifier is set up, we input our image to the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, img_class = classifier.load_image(\"photo.jpg\")\n",
    "\n",
    "# 1st convolutional layer execution, having as input the image and the trained parameters (weights)\n",
    "conv0 = utils.conv_layer(img, conv0_W, stride=4)\n",
    "# The result in then quantized to 2 bits representation for the subsequent HW offload\n",
    "conv0 = utils.threshold(conv0, conv0_T)\n",
    "\n",
    "\n",
    "# Compute offloaded convolutional layers\n",
    "out_dim = net['merge4']['output_dim']\n",
    "out_ch = net['merge4']['output_channels']\n",
    "\n",
    "conv_output = classifier.get_accel_buffer(out_ch, out_dim);\n",
    "conv_input = classifier.prepare_buffer(conv0)\n",
    "\n",
    "classifier.inference(conv_input, conv_output)\n",
    "\n",
    "conv_output = classifier.postprocess_buffer(conv_output)\n",
    "\n",
    "\n",
    "# Normalize results\n",
    "fc_input = conv_output / np.max(conv_output)\n",
    "\n",
    "# FC Layer 0\n",
    "fc0_W = fc_weights['fc0/Wn']\n",
    "fc0_b = fc_weights['fc0/bn']\n",
    "\n",
    "fc0_out = utils.fully_connected(fc_input, fc0_W, fc0_b)\n",
    "fc0_out = utils.qrelu(fc0_out)\n",
    "fc0_out = utils.quantize(fc0_out, 2)\n",
    "\n",
    "# FC Layer 1\n",
    "fc1_W = fc_weights['fc1/Wn']\n",
    "fc1_b = fc_weights['fc1/bn']\n",
    "\n",
    "fc1_out = utils.fully_connected(fc0_out, fc1_W, fc1_b)\n",
    "fc1_out = utils.qrelu(fc1_out)\n",
    "\n",
    "# FC Layer 2\n",
    "fct_W = fc_weights['fct/W']\n",
    "fct_b = np.zeros((fct_W.shape[1], ))\n",
    "\n",
    "fct_out = utils.fully_connected(fc1_out, fct_W, fct_b)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Use Softmax to Estimate the Probability of the Top 5 Most Likely Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax\n",
    "out = utils.softmax(fct_out)\n",
    "\n",
    "# Top-5 results\n",
    "topn =  utils.get_topn_indexes(out, 5)  \n",
    "for k in topn: print(\"class:{0:>20}\\tprobability:{1:>8.2%}\".format(names[k].lower(), out[k]))\n",
    "\n",
    "#display results\n",
    "x_pos = np.arange(len(topn))\n",
    "plt.barh(x_pos, out[topn], height=0.4, color='g', zorder=3)\n",
    "plt.yticks(x_pos, [names[k] for k in topn])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlim([0,1])\n",
    "plt.grid(zorder=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Acknowledgment:\n",
    "The following links helped preparing this notebook:\n",
    "- https://abauville.medium.com/display-your-live-webcam-feed-in-a-jupyter-notebook-using-opencv-d01eb75921d1\n",
    "- https://www.youtube.com/watch?v=LopYA64KmdE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
